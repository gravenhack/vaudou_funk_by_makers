{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e5d219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/doha/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2025-11-26 14:14:04.384248: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 14:14:04.520675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-26 14:14:05.917003: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/doha/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:827: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "from audiocraft.modules.conditioners import ConditioningAttributes\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92154988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration:\n",
      " Dataset: ./dataset/agbadja\n",
      " Outputs: ./musicgen_outputs\n",
      " Model: ./musicgen_agbadja_model\n",
      " Epochs: 5\n",
      " Learning Rate: 1e-05\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"./dataset/agbadja\"\n",
    "OUTPUT_PATH = \"./musicgen_outputs\"\n",
    "MODEL_SAVE_PATH = \"./musicgen_agbadja_model\"\n",
    "LOG_FILE = \"./training_log.json\"\n",
    "\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5  \n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "SAVE_EVERY = 2\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "print(\" Configuration:\")\n",
    "print(f\" Dataset: {DATASET_PATH}\")\n",
    "print(f\" Outputs: {OUTPUT_PATH}\")\n",
    "print(f\" Model: {MODEL_SAVE_PATH}\")\n",
    "print(f\" Epochs: {EPOCHS}\")\n",
    "print(f\" Learning Rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbf4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Metadata charg√©: 16 entr√©es\n",
      "\n",
      "16 descriptions charg√©es\n",
      "\n",
      " 16 fichiers audio trouv√©s dans ./dataset/agbadja\n",
      "\n",
      "    1. agbadja_1.wav\n",
      "        traditional Agbadja instrumental with energetic percussions ...\n",
      "    2. agbadja_10.wav\n",
      "        agbadja instrumental with layered percussion and steady cult...\n",
      "    3. agbadja_11.wav\n",
      "        Agbadja rhythm instrumental focused on drums and traditional...\n",
      "    4. agbadja_12.wav\n",
      "        deep traditional Agbadja groove with evolving rhythmic textu...\n",
      "    5. agbadja_13.wav\n",
      "        instrumental Agbadja beat highlighting dynamic drum sequence...\n",
      "    6. agbadja_14.wav\n",
      "        Agbadja percussive progression with cultural rhythmic identi...\n",
      "    7. agbadja_15.wav\n",
      "        traditional Agbadja instrumental loop with warm percussive t...\n",
      "    8. agbadja_16.wav\n",
      "        authentic Agbadja rhythm with layered percussion and stable ...\n",
      "    9. agbadja_2.wav\n",
      "        Agbadja style instrumental featuring steady drums and melodi...\n",
      "   10. agbadja_3.wav\n",
      "        traditional Agbadja groove recorded with percussive texture ...\n",
      "   11. agbadja_4.wav\n",
      "        Agbadja instrumental loop with steady tempo and ancestral pe...\n",
      "   12. agbadja_5.wav\n",
      "        deep Agbadja percussive ensemble creating a traditional rhyt...\n",
      "   13. agbadja_6.wav\n",
      "        authentic Agbadja instrumental pattern with vibrant rhythmic...\n",
      "   14. agbadja_7.wav\n",
      "        Agbadja rhythm section emphasizing hand drums and traditiona...\n",
      "   15. agbadja_8.wav\n",
      "        percussive Agbadja instrumentation with energetic pulse and ...\n",
      "   16. agbadja_9.wav\n",
      "        traditional Benin Agbadja melody played with rhythmic drum f...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_path = \"./dataset/metadata.csv\"\n",
    "\n",
    "try:\n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    print(f\" Metadata charg√©: {len(metadata_df)} entr√©es\\n\")\n",
    "    \n",
    "    audio_captions = {}\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        filename = Path(row['audio_path']).name\n",
    "        audio_captions[filename] = row['caption']\n",
    "    \n",
    "    print(f\"{len(audio_captions)} descriptions charg√©es\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"  Erreur chargement metadata: {e}\")\n",
    "    audio_captions = {}\n",
    "\n",
    "# Charger les fichiers audio\n",
    "def load_audio_files(dataset_path):\n",
    "    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg']\n",
    "    audio_files = []\n",
    "    \n",
    "    for ext in audio_extensions:\n",
    "        audio_files.extend(list(Path(dataset_path).glob(f'*{ext}')))\n",
    "    \n",
    "    return sorted(audio_files)\n",
    "\n",
    "audio_files = load_audio_files(DATASET_PATH)\n",
    "print(f\" {len(audio_files)} fichiers audio trouv√©s dans {DATASET_PATH}\\n\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    raise ValueError(f\" Aucun fichier audio dans {DATASET_PATH}\")\n",
    "\n",
    "# Afficher les fichiers avec leurs descriptions\n",
    "for i, file in enumerate(audio_files, 1):\n",
    "    caption = audio_captions.get(file.name, \"(pas de description)\")\n",
    "    print(f\"   {i:2d}. {file.name}\")\n",
    "    print(f\"        {caption[:60]}...\" if len(caption) > 60 else f\"        {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b07b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " EXEMPLES DE DESCRIPTIONS (metadata.csv):\n",
      "\n",
      "======================================================================\n",
      "\n",
      "1. Fichier: agbadja_1.wav\n",
      "   Description: \"traditional Agbadja instrumental with energetic percussions and rhythmic patterns\"\n",
      "\n",
      "2. Fichier: agbadja_2.wav\n",
      "   Description: \"Agbadja style instrumental featuring steady drums and melodic rhythmic flow\"\n",
      "\n",
      "3. Fichier: agbadja_3.wav\n",
      "   Description: \"traditional Agbadja groove recorded with percussive texture and cultural rhythm\"\n",
      "\n",
      "4. Fichier: agbadja_4.wav\n",
      "   Description: \"Agbadja instrumental loop with steady tempo and ancestral percussive elements\"\n",
      "\n",
      "5. Fichier: agbadja_5.wav\n",
      "   Description: \"deep Agbadja percussive ensemble creating a traditional rhythmic atmosphere\"\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\" EXEMPLES DE DESCRIPTIONS (metadata.csv):\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (filename, caption) in enumerate(list(audio_captions.items())[:5], 1):\n",
    "    print(f\"\\n{i}. Fichier: {filename}\")\n",
    "    print(f\"   Description: \\\"{caption}\\\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2d42b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analyse de 16 fichiers...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:01<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Statistiques du dataset:\n",
      "   Dur√©e moyenne: 30.00s\n",
      "   Dur√©e min: 30.00s\n",
      "   Dur√©e max: 30.04s\n",
      "   Sample rate moyen: 44100 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def analyze_audio_dataset(audio_files, max_files=None):\n",
    "    durations = []\n",
    "    sample_rates = []\n",
    "    \n",
    "    files_to_analyze = audio_files if max_files is None else audio_files[:max_files]\n",
    "    print(f\" Analyse de {len(files_to_analyze)} fichiers...\\n\")\n",
    "    \n",
    "    for audio_file in tqdm(files_to_analyze, desc=\"Analyse\"):\n",
    "        try:\n",
    "            y, sr = librosa.load(str(audio_file), sr=None, mono=True)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            durations.append(duration)\n",
    "            sample_rates.append(sr)\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur avec {audio_file.name}: {e}\")\n",
    "    \n",
    "    if durations:\n",
    "        print(f\"\\n Statistiques du dataset:\")\n",
    "        print(f\"   Dur√©e moyenne: {np.mean(durations):.2f}s\")\n",
    "        print(f\"   Dur√©e min: {np.min(durations):.2f}s\")\n",
    "        print(f\"   Dur√©e max: {np.max(durations):.2f}s\")\n",
    "        print(f\"   Sample rate moyen: {int(np.mean(sample_rates))} Hz\")\n",
    "        return np.mean(durations), int(np.mean(sample_rates))\n",
    "    return None, None\n",
    "\n",
    "avg_duration, avg_sr = analyze_audio_dataset(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0634273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement de MusicGen-small...\n",
      " Mod√®le charg√© avec succ√®s\n",
      "\n",
      " Device: cpu\n",
      "Param√®tres totaux: 420,371,456\n",
      "Param√®tres entra√Ænables: 420,371,456\n"
     ]
    }
   ],
   "source": [
    "MODEL_SIZE = 'small'  \n",
    "\n",
    "print(f\" Chargement de MusicGen-{MODEL_SIZE}...\")\n",
    "model = MusicGen.get_pretrained(f'facebook/musicgen-{MODEL_SIZE}')\n",
    "print(f\" Mod√®le charg√© avec succ√®s\\n\")\n",
    "\n",
    "# Configuration du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.lm.to(device)\n",
    "print(f\" Device: {device}\")\n",
    "\n",
    "# Param√®tres du mod√®le\n",
    "total_params = sum(p.numel() for p in model.lm.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.lm.parameters() if p.requires_grad)\n",
    "print(f\"Param√®tres totaux: {total_params:,}\")\n",
    "print(f\"Param√®tres entra√Ænables: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd74423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dataset cr√©√©:\n",
      "   Fichiers: 16\n",
      "   Captions: 16\n",
      "   Sample Rate: 32000 Hz\n",
      "   Dur√©e par segment: 10s\n",
      " DataLoader pr√™t (16 samples avec captions)\n"
     ]
    }
   ],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, audio_captions, target_sr=32000, duration=10):\n",
    "        self.audio_files = audio_files\n",
    "        self.audio_captions = audio_captions\n",
    "        self.target_sr = target_sr\n",
    "        self.duration = duration\n",
    "        self.target_length = int(target_sr * duration)\n",
    "        \n",
    "        print(f\"   Dataset cr√©√©:\")\n",
    "        print(f\"   Fichiers: {len(audio_files)}\")\n",
    "        print(f\"   Captions: {len(audio_captions)}\")\n",
    "        print(f\"   Sample Rate: {target_sr} Hz\")\n",
    "        print(f\"   Dur√©e par segment: {duration}s\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = str(self.audio_files[idx])\n",
    "        filename = self.audio_files[idx].name\n",
    "        caption = self.audio_captions.get(filename, \"\")\n",
    "        \n",
    "        try:\n",
    "            # Charger l'audio\n",
    "            waveform_np, sr = librosa.load(audio_path, sr=self.target_sr, mono=True)\n",
    "            waveform = torch.from_numpy(waveform_np).float().unsqueeze(0)\n",
    "            \n",
    "            # Ajuster la longueur\n",
    "            if waveform.shape[1] > self.target_length:\n",
    "                # D√©couper al√©atoirement\n",
    "                start = np.random.randint(0, waveform.shape[1] - self.target_length)\n",
    "                waveform = waveform[:, start:start + self.target_length]\n",
    "            elif waveform.shape[1] < self.target_length:\n",
    "                # Padding\n",
    "                padding = self.target_length - waveform.shape[1]\n",
    "                waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "            \n",
    "            return waveform, caption\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Erreur: {filename}: {e}\")\n",
    "            return torch.zeros(1, self.target_length), \"\"\n",
    "\n",
    "# Cr√©er le dataset et dataloader\n",
    "dataset = AudioDataset(audio_files, audio_captions, target_sr=32000, duration=10)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "print(f\" DataLoader pr√™t ({len(dataset)} samples avec captions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40711eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TEST DE LA BOUCLE D'ENTRA√éNEMENT\n",
      "======================================================================\n",
      " Batch de test charg√©: torch.Size([1, 1, 320000])\n",
      " Caption: traditional Agbadja instrumental loop with warm percussive tones...\n",
      " Codes encod√©s: torch.Size([1, 4, 500])\n",
      " Logits g√©n√©r√©s: torch.Size([1, 4, 500, 2048])\n",
      " Loss calcul√©e: 3.8217\n",
      "\n",
      "======================================================================\n",
      " TEST R√âUSSI! La boucle d'entra√Ænement fonctionne correctement\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\" TEST DE LA BOUCLE D'ENTRA√éNEMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Pr√©parer le mod√®le\n",
    "    model.compression_model.eval()\n",
    "    for param in model.compression_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.lm.train()\n",
    "    \n",
    "    # Test avec un seul batch\n",
    "    test_batch, test_caption = next(iter(dataloader))\n",
    "    test_batch = test_batch.to(device)\n",
    "    print(f\" Batch de test charg√©: {test_batch.shape}\")\n",
    "    print(f\" Caption: {test_caption[0][:80]}...\")\n",
    "    \n",
    "    # Encoder\n",
    "    with torch.no_grad():\n",
    "        encoded = model.compression_model.encode(test_batch)\n",
    "        codes = encoded[0] if isinstance(encoded, tuple) else encoded\n",
    "        \n",
    "        if codes.dim() > 2:\n",
    "            codes = codes[0].unsqueeze(0)\n",
    "    \n",
    "    print(f\" Codes encod√©s: {codes.shape}\")\n",
    "    \n",
    "    # Cr√©er l'objet de conditioning\n",
    "    conditioning = ConditioningAttributes(text={'description': test_caption[0]})\n",
    "    \n",
    "    # Forward pass AVEC conditioning (text)\n",
    "    lm_output = model.lm.compute_predictions(\n",
    "        codes=codes,\n",
    "        conditions=[conditioning]\n",
    "    )\n",
    "    \n",
    "    # Extraire les logits de l'objet LMOutput\n",
    "    logits = lm_output.logits\n",
    "    \n",
    "    print(f\" Logits g√©n√©r√©s: {logits.shape}\")\n",
    "    \n",
    "    # Loss - logits: [B, K, T, vocab_size], codes: [B, K, T]\n",
    "    # On calcule la loss sur la premi√®re codebook (K=0)\n",
    "    B, K, T, vocab_size = logits.shape\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits[:, 0, :, :].reshape(-1, vocab_size),  # [B*T, vocab_size]\n",
    "        codes[:, 0, :].reshape(-1)                     # [B*T]\n",
    "    )\n",
    "    \n",
    "    print(f\" Loss calcul√©e: {loss.item():.4f}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" TEST R√âUSSI! La boucle d'entra√Ænement fonctionne correctement\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Nettoyer\n",
    "    del test_batch, codes, logits, loss\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n ERREUR: {e}\")\n",
    "\n",
    "\n",
    "    import traceback        \n",
    "\n",
    "    traceback.print_exc()    \n",
    "    print(\"\\n V√©rifiez les erreurs ci-dessus avant de lancer l'entra√Ænement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d375854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pr√™t √† lancer l'entra√Ænement\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    training_log = {\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'model': MODEL_SIZE,\n",
    "            'epochs': EPOCHS,\n",
    "            'learning_rate': LEARNING_RATE,\n",
    "            'gradient_accumulation': GRADIENT_ACCUMULATION_STEPS,\n",
    "            'num_files': len(audio_files),\n",
    "            'device': str(device)\n",
    "        },\n",
    "        'epochs': []\n",
    "    }\n",
    "    \n",
    "    # Configuration du mod√®le\n",
    "    model.compression_model.eval()\n",
    "    for param in model.compression_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.lm.train()\n",
    "    \n",
    "    # Optimiseur\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.lm.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    print(f\" Configuration pr√™te\")\n",
    "    print(f\"   Mod√®le: MusicGen-{MODEL_SIZE}\")\n",
    "    print(f\"   Epochs: {EPOCHS}\")\n",
    "    print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   Device: {device}\\n\")\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(EPOCHS):\n",
    "            epoch_loss = 0\n",
    "            batch_count = 0\n",
    "            successful_batches = 0\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\" EPOCH {epoch + 1}/{EPOCHS}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            \n",
    "            pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\", ncols=100)\n",
    "            \n",
    "            for batch_idx, batch_data in enumerate(pbar):\n",
    "                try:\n",
    "                    audio_batch, captions = batch_data\n",
    "                    audio_batch = audio_batch.to(device)\n",
    "                    \n",
    "                    # Encoder l'audio\n",
    "                    with torch.no_grad():\n",
    "                        encoded = model.compression_model.encode(audio_batch)\n",
    "                        codes = encoded[0] if isinstance(encoded, tuple) else encoded\n",
    "                        \n",
    "                        if codes.dim() > 2:\n",
    "                            codes = codes[0].unsqueeze(0)\n",
    "                    \n",
    "                    if codes.shape[0] == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Utiliser le caption du metadata.csv pour conditioning\n",
    "                    B, K, T = codes.shape\n",
    "                    caption = captions[0] if len(captions) > 0 else \"\"\n",
    "                    \n",
    "                    # Cr√©er l'objet de conditioning\n",
    "                    conditioning = ConditioningAttributes(text={'description': caption})\n",
    "                    \n",
    "                    # Forward pass AVEC le texte de description\n",
    "                    lm_output = model.lm.compute_predictions(\n",
    "                        codes=codes,\n",
    "                        conditions=[conditioning]\n",
    "                    )\n",
    "                    \n",
    "                    # Extraire les logits de l'objet LMOutput\n",
    "                    logits = lm_output.logits\n",
    "                    \n",
    "                    # Loss - logits: [B, K, T, vocab_size], codes: [B, K, T]\n",
    "                    # On calcule la loss sur la premi√®re codebook (K=0)\n",
    "                    B_train, K_train, T_train, vocab_size_train = logits.shape\n",
    "                    loss = torch.nn.functional.cross_entropy(\n",
    "                        logits[:, 0, :, :].reshape(-1, vocab_size_train),  # [B*T, vocab_size]\n",
    "                        codes[:, 0, :].reshape(-1)                           # [B*T]\n",
    "                    )\n",
    "                    \n",
    "                    loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    # Update\n",
    "                    if (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.lm.parameters(),\n",
    "                            max_norm=MAX_GRAD_NORM\n",
    "                        )\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    # Tracking\n",
    "                    epoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "                    batch_count += 1\n",
    "                    successful_batches += 1\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}',\n",
    "                        'success': f'{successful_batches}/{len(dataloader)}'\n",
    "                    })\n",
    "                    \n",
    "                    # Nettoyage m√©moire\n",
    "                    del audio_batch, codes, logits, loss\n",
    "                    if batch_idx % 5 == 0:\n",
    "                        gc.collect()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n Erreur batch {batch_idx}: {str(e)}\")\n",
    "                    import traceback\n",
    "                    print(f\"D√©tails: {traceback.format_exc()[:500]}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "            \n",
    "            # Statistiques epoch\n",
    "            avg_loss = epoch_loss / batch_count if batch_count > 0 else float('inf')\n",
    "            \n",
    "            epoch_stats = {\n",
    "                'epoch': epoch + 1,\n",
    "                'avg_loss': avg_loss,\n",
    "                'successful_batches': successful_batches,\n",
    "                'total_batches': len(dataloader)\n",
    "            }\n",
    "            training_log['epochs'].append(epoch_stats)\n",
    "            \n",
    "            print(f\"\\n Epoch {epoch + 1} termin√©\")\n",
    "            print(f\"   Loss moyenne: {avg_loss:.4f}\")\n",
    "            print(f\"   Batches r√©ussis: {successful_batches}/{len(dataloader)}\")\n",
    "            \n",
    "            # Sauvegarder checkpoint\n",
    "            if (epoch + 1) % SAVE_EVERY == 0:\n",
    "                checkpoint_path = os.path.join(\n",
    "                    MODEL_SAVE_PATH,\n",
    "                    f'checkpoint_epoch_{epoch + 1}.pt'\n",
    "                )\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': model.lm.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                    'config': training_log['config']\n",
    "                }, checkpoint_path)\n",
    "                \n",
    "                print(f\" Checkpoint sauvegard√©: checkpoint_epoch_{epoch + 1}.pt\")\n",
    "            \n",
    "            # Sauvegarder le log\n",
    "            with open(LOG_FILE, 'w') as f:\n",
    "                json.dump(training_log, f, indent=2)\n",
    "        \n",
    "        # Sauvegarder le mod√®le final\n",
    "        final_path = os.path.join(MODEL_SAVE_PATH, 'final_model.pt')\n",
    "        torch.save({\n",
    "            'model_state_dict': model.lm.state_dict(),\n",
    "            'training_log': training_log\n",
    "        }, final_path)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\" ENTRA√éNEMENT TERMIN√â AVEC SUCC√àS\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\" Mod√®le final: {final_path}\")\n",
    "        print(f\" Log: {LOG_FILE}\")\n",
    "        \n",
    "        return training_log\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n  Entra√Ænement interrompu par l'utilisateur\")\n",
    "        return training_log\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERREUR CRITIQUE: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return training_log\n",
    "\n",
    "\n",
    "\n",
    "print(\" Pr√™t √† lancer l'entra√Ænement\")# LANCER L'ENTRA√éNEMENTprint(\" Ex√©cutez cette cellule pour commencer\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23832fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration pr√™te\n",
      "   Mod√®le: MusicGen-small\n",
      "   Epochs: 5\n",
      "   Learning Rate: 1e-05\n",
      "   Device: cpu\n",
      "\n",
      "\n",
      "======================================================================\n",
      " EPOCH 1/5\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                      | 3/16 [00:18<01:21,  6.28s/it, loss=2.4129, success=3/16]"
     ]
    }
   ],
   "source": [
    "training_log = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a43f2",
   "metadata": {},
   "source": [
    "---\n",
    "## üéµ G√âN√âRATION DE MUSIQUE AVEC PROMPTS PERSONNALIS√âS\n",
    "\n",
    "Maintenant que le mod√®le est entra√Æn√©, vous pouvez g√©n√©rer de la musique en utilisant des prompts personnalis√©s.\n",
    "Le mod√®le a appris le style Agbadja et peut cr√©er des variations bas√©es sur vos descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91627ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéº FONCTION DE G√âN√âRATION AVEC PROMPT PERSONNALIS√â\n",
    "def generate_music(prompt, duration=10, temperature=0.8, top_k=200, cfg_coef=4.0, output_name=\"generation\"):\n",
    "    \"\"\"\n",
    "    G√©n√®re de la musique bas√©e sur un prompt personnalis√©\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Description de la musique √† g√©n√©rer\n",
    "        duration (int): Dur√©e en secondes (d√©faut: 10)\n",
    "        temperature (float): Cr√©ativit√© (0.5=conservateur, 1.0=cr√©atif) (d√©faut: 0.8)\n",
    "        top_k (int): Diversit√© des choix (100-250) (d√©faut: 200)\n",
    "        cfg_coef (float): Force du prompt (3.0-6.0) (d√©faut: 4.0)\n",
    "        output_name (str): Nom du fichier de sortie\n",
    "    \n",
    "    Returns:\n",
    "        str: Chemin du fichier audio g√©n√©r√©\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéµ G√âN√âRATION DE MUSIQUE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"‚è±Ô∏è  Dur√©e: {duration}s\")\n",
    "    print(f\"üå°Ô∏è  Temperature: {temperature}\")\n",
    "    print(f\"üé≤ Top-K: {top_k}\")\n",
    "    print(f\"üéØ CFG Coefficient: {cfg_coef}\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # Mettre le mod√®le en mode √©valuation\n",
    "    model.lm.eval()\n",
    "    \n",
    "    # Configurer les param√®tres de g√©n√©ration\n",
    "    model.set_generation_params(\n",
    "        duration=duration,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=0.0,\n",
    "        cfg_coef=cfg_coef\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # G√©n√©rer\n",
    "            print(\"‚è≥ G√©n√©ration en cours...\")\n",
    "            wav = model.generate(\n",
    "                descriptions=[prompt],\n",
    "                progress=True\n",
    "            )\n",
    "            \n",
    "            # Sauvegarder\n",
    "            output_path = os.path.join(OUTPUT_PATH, output_name)\n",
    "            audio_write(\n",
    "                output_path,\n",
    "                wav[0].cpu(),\n",
    "                model.sample_rate,\n",
    "                strategy=\"loudness\",\n",
    "                loudness_compressor=True\n",
    "            )\n",
    "            \n",
    "            output_file = output_path + '.wav'\n",
    "            file_size_kb = os.path.getsize(output_file) / 1024\n",
    "            \n",
    "            print(f\"\\n‚úÖ SUCC√àS!\")\n",
    "            print(f\"üìÅ Fichier: {output_name}.wav ({file_size_kb:.1f} KB)\")\n",
    "            print(f\"üìÇ Dossier: {OUTPUT_PATH}/\")\n",
    "            \n",
    "            # Nettoyer\n",
    "            del wav\n",
    "            gc.collect()\n",
    "            \n",
    "            return output_file\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERREUR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Fonction generate_music() pr√™te\")\n",
    "print(\"\\nüí° Exemple d'utilisation:\")\n",
    "print('   generate_music(\"traditional Agbadja drums with energetic rhythm\", duration=10)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4803578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® EXEMPLES DE G√âN√âRATION\n",
    "\n",
    "# Exemple 1: Style traditionnel pur\n",
    "print(\"ü•Å Exemple 1: Style traditionnel Agbadja\")\n",
    "file1 = generate_music(\n",
    "    prompt=\"traditional Agbadja percussion, West African ceremonial drums, djembe and talking drum ensemble\",\n",
    "    duration=10,\n",
    "    temperature=0.7,\n",
    "    output_name=\"agbadja_traditional\"\n",
    ")\n",
    "\n",
    "# Exemple 2: Avec √©nergie accrue\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"‚ö° Exemple 2: Style √©nergique\")\n",
    "file2 = generate_music(\n",
    "    prompt=\"energetic Agbadja drums with powerful polyrhythmic\",\n",
    "    duration=10,\n",
    "    temperature=0.8,\n",
    "    output_name=\"agbadja_energetic\"\n",
    ")\n",
    "\n",
    "# Exemple 3: Ambiance c√©r√©monielle\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"üåô Exemple 3: Ambiance c√©r√©monielle\")\n",
    "file3 = generate_music(\n",
    "    prompt=\"ceremonial Agbadja ritual drums, deep resonant tones, ancestral rhythm patterns\",\n",
    "    duration=10,\n",
    "    temperature=0.7,\n",
    "    output_name=\"agbadja_ceremonial\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ 3 EXEMPLES G√âN√âR√âS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üìÅ Fichiers dans {OUTPUT_PATH}/\")\n",
    "print(\"   1. agbadja_traditional.wav\")\n",
    "print(\"   2. agbadja_energetic.wav\")\n",
    "print(\"   3. agbadja_ceremonial.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéß √âCOUTER LES G√âN√âRATIONS\n",
    "print(\"üéß √âCOUTE DES G√âN√âRATIONS\\n\")\n",
    "\n",
    "# Lister tous les fichiers g√©n√©r√©s\n",
    "generated_files = sorted(Path(OUTPUT_PATH).glob('*.wav'))\n",
    "\n",
    "if len(generated_files) == 0:\n",
    "    print(\"‚ùå Aucun fichier g√©n√©r√© trouv√©\")\n",
    "else:\n",
    "    print(f\"üìÅ {len(generated_files)} fichier(s) trouv√©(s) dans {OUTPUT_PATH}/\\n\")\n",
    "    \n",
    "    for i, filepath in enumerate(generated_files, 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"üéµ {i}. {filepath.name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Charger et afficher l'audio\n",
    "            y, sr = librosa.load(str(filepath), sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            file_size = filepath.stat().st_size / 1024\n",
    "            \n",
    "            print(f\"   Dur√©e: {duration:.2f}s\")\n",
    "            print(f\"   Taille: {file_size:.1f} KB\")\n",
    "            print(f\"   Sample Rate: {sr} Hz\\n\")\n",
    "            \n",
    "            display(Audio(y, rate=sr))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur de lecture: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ú® G√âN√âRATION PERSONNALIS√âE INTERACTIVE\n",
    "\n",
    "print(\"‚ú® G√âN√âRATION PERSONNALIS√âE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Entrez votre propre prompt pour g√©n√©rer de la musique!\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# PERSONNALISEZ ICI VOTRE PROMPT\n",
    "user_prompt = \"traditional Agbadja drums with fast tempo and complex polyrhythms\"\n",
    "\n",
    "# PARAM√àTRES AJUSTABLES\n",
    "duration = 10          # Dur√©e en secondes\n",
    "temperature = 0.8      # 0.5 = conservateur, 1.0 = cr√©atif\n",
    "top_k = 200           # 100-250, contr√¥le la diversit√©\n",
    "cfg_coef = 4.0        # 3.0-6.0, force du prompt\n",
    "output_name = \"custom_generation\"\n",
    "\n",
    "print(f\"üìù Votre prompt: {user_prompt}\")\n",
    "print(f\"‚öôÔ∏è  Param√®tres:\")\n",
    "print(f\"   - Dur√©e: {duration}s\")\n",
    "print(f\"   - Temperature: {temperature}\")\n",
    "print(f\"   - Top-K: {top_k}\")\n",
    "print(f\"   - CFG Coef: {cfg_coef}\\n\")\n",
    "\n",
    "# G√©n√©rer\n",
    "generated_file = generate_music(\n",
    "    prompt=user_prompt,\n",
    "    duration=duration,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    cfg_coef=cfg_coef,\n",
    "    output_name=output_name\n",
    ")\n",
    "\n",
    "# √âcouter\n",
    "if generated_file:\n",
    "    print(\"\\nüéß √âcoute du r√©sultat:\")\n",
    "    y, sr = librosa.load(generated_file, sr=None)\n",
    "    display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6706fa9",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä VISUALISATION DES R√âSULTATS D'ENTRA√éNEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e5cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà COURBE D'ENTRA√éNEMENT\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if training_log and len(training_log.get('epochs', [])) > 0:\n",
    "    losses = [e['avg_loss'] for e in training_log['epochs']]\n",
    "    epochs_nums = [e['epoch'] for e in training_log['epochs']]\n",
    "    success_rates = [e['successful_batches']/e['total_batches']*100 for e in training_log['epochs']]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Courbe de loss\n",
    "    ax1.plot(epochs_nums, losses, marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('√âvolution de la Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Taux de r√©ussite\n",
    "    ax2.plot(epochs_nums, success_rates, marker='s', linewidth=2, markersize=8, color='#06A77D')\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Taux de r√©ussite (%)', fontsize=12)\n",
    "    ax2.set_title('Taux de r√©ussite des batches', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_path = os.path.join(OUTPUT_PATH, 'training_curves.png')\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Statistiques d'entra√Ænement:\")\n",
    "    print(f\"   Loss initiale: {losses[0]:.4f}\")\n",
    "    print(f\"   Loss finale: {losses[-1]:.4f}\")\n",
    "    if losses[0] > 0:\n",
    "        improvement = ((losses[0] - losses[-1]) / losses[0]) * 100\n",
    "        print(f\"   Am√©lioration: {improvement:.2f}%\")\n",
    "    print(f\"   Taux de r√©ussite final: {success_rates[-1]:.1f}%\")\n",
    "    print(f\"\\nüíæ Graphique sauvegard√©: {plot_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Aucune donn√©e d'entra√Ænement disponible\")\n",
    "    print(\"üí° Ex√©cutez d'abord la cellule d'entra√Ænement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369337d",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° GUIDE D'UTILISATION\n",
    "\n",
    "### üöÄ Comment utiliser ce notebook:\n",
    "\n",
    "1. **Ex√©cutez les cellules 1-7** pour charger le dataset et le mod√®le\n",
    "2. **Cellule 8**: Lance l'entra√Ænement (peut prendre plusieurs heures)\n",
    "3. **Cellule 10-11**: G√©n√®re des exemples avec prompts pr√©d√©finis\n",
    "4. **Cellule 12**: Personnalisez votre prompt pour cr√©er votre propre musique\n",
    "\n",
    "### üé® Conseils pour les prompts:\n",
    "\n",
    "**√âl√©ments √† inclure:**\n",
    "- Style musical: \"traditional Agbadja\", \"ceremonial drums\"\n",
    "- Instruments: \"djembe\", \"talking drums\", \"dundun\"\n",
    "- Tempo: \"fast\", \"slow\", \"steady\"\n",
    "- Ambiance: \"energetic\", \"calm\", \"powerful\", \"ritual\"\n",
    "- Patterns: \"polyrhythmic\", \"complex patterns\", \"simple rhythm\"\n",
    "\n",
    "**Exemples de bons prompts:**\n",
    "```\n",
    "\"traditional Agbadja percussion with fast polyrhythmic djembe patterns\"\n",
    "\"ceremonial West African drums, deep dundun bass with talking drum accents\"\n",
    "\"energetic Agbadja ensemble, complex polyrhythms, powerful ceremonial energy\"\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Param√®tres de g√©n√©ration:\n",
    "\n",
    "- **temperature** (0.5-1.0): Plus bas = plus fid√®le au dataset, plus haut = plus cr√©atif\n",
    "- **top_k** (100-250): Contr√¥le la diversit√© des choix\n",
    "- **cfg_coef** (3.0-6.0): Force avec laquelle le mod√®le suit le prompt\n",
    "- **duration**: Dur√©e en secondes (recommand√©: 10-30s)\n",
    "\n",
    "### üìÇ Fichiers g√©n√©r√©s:\n",
    "\n",
    "- **Mod√®le**: `./musicgen_agbadja_model/final_model.pt`\n",
    "- **Checkpoints**: `./musicgen_agbadja_model/checkpoint_epoch_X.pt`\n",
    "- **G√©n√©rations**: `./musicgen_outputs/*.wav`\n",
    "- **Log**: `./training_log.json`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
